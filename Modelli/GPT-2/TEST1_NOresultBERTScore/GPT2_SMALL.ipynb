{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2bnSzaIiR3iU",
        "outputId": "4b0d5796-38b0-402c-fe9d-2f4e929ad1c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 16.1.0 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 16.1.0 which is incompatible.\n",
            "ludwig 0.10.3.dev0 requires pyarrow<15.0.0, but you have pyarrow 16.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mFound existing installation: torch 2.3.0\n",
            "Uninstalling torch-2.3.0:\n",
            "  Successfully uninstalled torch-2.3.0\n",
            "Found existing installation: torchdata 0.5.1\n",
            "Uninstalling torchdata-0.5.1:\n",
            "  Successfully uninstalled torchdata-0.5.1\n",
            "Collecting torch==1.13.1\n",
            "  Using cached torch-1.13.1-cp310-cp310-manylinux1_x86_64.whl (887.5 MB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.13.1) (4.12.2)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==1.13.1) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch==1.13.1) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch==1.13.1) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==1.13.1) (11.7.99)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1) (67.7.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1) (0.43.0)\n",
            "Installing collected packages: torch\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ludwig 0.10.3.dev0 requires pyarrow<15.0.0, but you have pyarrow 16.1.0 which is incompatible.\n",
            "ludwig 0.10.3.dev0 requires torch>=2.0.0, but you have torch 1.13.1 which is incompatible.\n",
            "torchaudio 2.3.0+cu121 requires torch==2.3.0, but you have torch 1.13.1 which is incompatible.\n",
            "torchtext 0.18.0 requires torch>=2.3.0, but you have torch 1.13.1 which is incompatible.\n",
            "torchvision 0.18.0+cu121 requires torch==2.3.0, but you have torch 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-1.13.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch",
                  "torchgen"
                ]
              },
              "id": "a031c1916ab3410a8e5fd60d2e3de90e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchdata==0.5.1\n",
            "  Using cached torchdata-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.5.1) (1.26.19)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchdata==0.5.1) (2.32.3)\n",
            "Requirement already satisfied: portalocker>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.5.1) (2.10.0)\n",
            "Requirement already satisfied: torch==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.5.1) (1.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.13.1->torchdata==0.5.1) (4.12.2)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==1.13.1->torchdata==0.5.1) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch==1.13.1->torchdata==0.5.1) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch==1.13.1->torchdata==0.5.1) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==1.13.1->torchdata==0.5.1) (11.7.99)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1->torchdata==0.5.1) (67.7.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1->torchdata==0.5.1) (0.43.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata==0.5.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata==0.5.1) (3.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata==0.5.1) (2024.6.2)\n",
            "Installing collected packages: torchdata\n",
            "Successfully installed torchdata-0.5.1\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install bitsandbytes>=0.41.3\n",
        "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
        "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
        "!pip install -q -U datasets scipy ipywidgets\n",
        "!pip uninstall torch torchdata -y\n",
        "!pip install torch==1.13.1\n",
        "!pip install torchdata==0.5.1\n",
        "!pip install git+https://github.com/ludwig-ai/ludwig.git@master --quiet\n",
        "!pip install bert-score\n",
        "!pip install ludwig\n",
        "!pip install nlpaug"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gc\n",
        "\n",
        "from google.colab import data_table\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "from torch import Tensor\n",
        "\n",
        "import datasets\n",
        "from datasets import Dataset, DatasetDict\n",
        "\n",
        "import yaml\n",
        "from ludwig.api import LudwigModel, TrainingResults"
      ],
      "metadata": {
        "id": "7ijeE-AjTDhA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "uJRH0DbvTDii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "file_name = \"Output.csv\"\n",
        "try:\n",
        "    # Prova a leggere il file con il delimitatore corretto\n",
        "    df = pd.read_csv(file_name, delimiter=',', on_bad_lines='warn')\n",
        "except pd.errors.ParserError as e:\n",
        "    print(f\"Error parsing CSV file: {e}\")\n",
        "\n",
        "# Rimuove il carattere `;` dal nome della colonna\n",
        "df.columns = [col.strip(';') for col in df.columns]\n",
        "\n",
        "# Visualizza le prime righe del dataset\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "Z03JPOncTDmT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pulizia del dataset\n",
        "df_cleaned = df.dropna()"
      ],
      "metadata": {
        "id": "rMY0GVE6TDoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils import resample\n",
        "import nlpaug.augmenter.word as naw\n",
        "\n",
        "# Bilanciamento del dataset\n",
        "min_class_size = df_cleaned['DOMANDE'].value_counts().min()\n",
        "df_balanced = df_cleaned.groupby('DOMANDE').apply(lambda x: resample(x, replace=False, n_samples=min_class_size)).reset_index(drop=True)\n",
        "\n",
        "# Augmentazione dei dati\n",
        "aug = naw.SynonymAug(aug_src='wordnet')\n",
        "\n",
        "df_balanced['DOMANDE_aug'] = df_balanced['DOMANDE'].apply(lambda x: aug.augment(x))\n",
        "df_balanced['RISPOSTE_aug'] = df_balanced['RISPOSTE'].apply(lambda x: aug.augment(x))\n",
        "\n",
        "df_augmented = pd.concat([df_balanced[['DOMANDE', 'RISPOSTE']],\n",
        "                          df_balanced[['DOMANDE_aug', 'RISPOSTE_aug']].rename(columns={'DOMANDE_aug': 'DOMANDE', 'RISPOSTE_aug': 'RISPOSTE'})])"
      ],
      "metadata": {
        "id": "UqYnvGfbTjw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_augmented['DOMANDE'] = df_augmented['DOMANDE'].apply(lambda x: ' '.join(x) if isinstance(x, list) else str(x))\n",
        "df_augmented['RISPOSTE'] = df_augmented['RISPOSTE'].apply(lambda x: ' '.join(x) if isinstance(x, list) else str(x))"
      ],
      "metadata": {
        "id": "sbS71LadTjyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#suddivisione del set di dati in un set di addestramento, in uno di test e in uno di validazione\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#separo le caratteristiche dal target (divido input da output)\n",
        "df_train, df_temp = train_test_split(df_cleaned, test_size=0.3, random_state=200)\n",
        "df_test, df_validation = train_test_split(df_temp, test_size=0.1, random_state=200)"
      ],
      "metadata": {
        "id": "ItZEKR_3Tj2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verifica delle dimensioni\n",
        "print(f'Train set size: {df_train.shape}')\n",
        "print(f'Test set size: {df_test.shape}')\n",
        "print(f'Validation set size: {df_validation.shape}')"
      ],
      "metadata": {
        "id": "UKgMQk8oTj3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aggiungere una colonna per indicare lo split (per indicare quale riga appartiene a quale suddivisione.)\n",
        "df_train[\"split\"] = np.zeros(df_train.shape[0])\n",
        "df_test[\"split\"] = np.ones(df_test.shape[0])\n",
        "df_validation[\"split\"] = np.full(df_validation.shape[0], 2)\n",
        "\n",
        "df_dataset = pd.concat([df_train, df_test, df_validation])\n",
        "df_dataset[\"split\"] = df_dataset[\"split\"].astype(int)\n",
        "\n",
        "# Verifica delle dimensioni complessive\n",
        "print(df_dataset.shape)"
      ],
      "metadata": {
        "id": "VcoExtDITu2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "# Percorso alla cartella del modello salvato\n",
        "model_path: str = \"GroNLP/gpt2-small-italian\"\n",
        "\n",
        "# Carica il tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_path,\n",
        "    model_max_length=512,\n",
        "    trust_remote_code=True,\n",
        "    padding_side=\"left\",\n",
        "    add_eos_token=True)\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    load_in_8bit=False,\n",
        "    llm_int8_threshold=6.0,\n",
        "    llm_int8_has_fp16_weight=False,\n",
        "    bnb_4bit_compute_dtype=\"float16\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        ")\n",
        "\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_path,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16,\n",
        "    offload_folder=\"offload\", # Offload di parti del modello sulla CPU\n",
        "    trust_remote_code=True,\n",
        "    low_cpu_mem_usage=True,\n",
        "   quantization_config=bnb_config\n",
        ")"
      ],
      "metadata": {
        "id": "PpO3NECVTu4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Estrazione delle prime 10 domande dal DataFrame originale\n",
        "eval_df = df_test[['DOMANDE', 'RISPOSTE']].head(3)\n",
        "# Estrai le risposte di riferimento\n",
        "reference_responses = eval_df['RISPOSTE'].tolist()"
      ],
      "metadata": {
        "id": "FDmgzW8ATu67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Funzione per assicurarsi che le frasi termino con u punto\n",
        "def ensure_ending_with_period(response):\n",
        "    response = response.strip()\n",
        "    if not response.endswith('.'):\n",
        "       response += '.'\n",
        "    return response"
      ],
      "metadata": {
        "id": "xCi8iq0lTu8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template: str = \"\"\"\n",
        "#Genera una risposta per la seguente domanda, come se fossi un esperto cardiologo.\n",
        "#Le risposte devono essere empatiche, di senso compiuto e LE FRASI DEVONO ESSERE LUNGHE MASSIMO 60 TOKEN E TERMINARE CON UN PUNTO.\n",
        "\n",
        "### Domanda: {DOMANDE}\n",
        "\n",
        "### Risposta:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "w8ZwHpV_TvAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Funzione per generare risposte e calcolare BERTScore\n",
        "def generate_responses_pre_finetuning(model, tokenizer, eval_df):\n",
        "  #PROVO CON LA PIPELINE DI GENERAZIONE TESTO (Creo la pipeline di generazione)\n",
        "  response_generator = pipeline(\n",
        "      task=\"text-generation\",\n",
        "      model=model,\n",
        "      tokenizer=tokenizer,\n",
        "      torch_dtype=torch.float16,\n",
        "      device_map=\"auto\"\n",
        "  )\n",
        "\n",
        "  # Genera e stampa la risposta per le prime 10 domande\n",
        "  generated_responses = []\n",
        "  for index, row in eval_df.iterrows():\n",
        "    domande_test = row['DOMANDE']\n",
        "    print(f'Generating response for question {index + 1}/{len(eval_df)}: {domande_test}')\n",
        "    test_prompt = prompt_template.format(DOMANDE=domande_test)\n",
        "    try:\n",
        "      responses = response_generator(\n",
        "          text_inputs=test_prompt,\n",
        "          do_sample=True,\n",
        "          top_k=50,\n",
        "          num_return_sequences=1,\n",
        "          eos_token_id=tokenizer.eos_token_id,\n",
        "          max_length=512,\n",
        "          truncation=True,\n",
        "          #return_text=True\n",
        "      )\n",
        "      print(f'Pipeline output for question {index + 1}: {responses}')\n",
        "      # Estrai e stampa la risposta generata\n",
        "      generated_response = responses[0]['generated_text']\n",
        "      generated_response = ensure_ending_with_period(generated_response)\n",
        "      generated_responses.append(generated_response)\n",
        "      print(f'\\n[DOMANDA]: {domande_test}\\n[GENERATED_TEXT] MODEL_PREDICTION:\\n{generated_response}\\n')\n",
        "    except Exception as e:\n",
        "      print(f\"Error generating response for question {index + 1}: {e}\")\n",
        "    print(f\"Generated {len(generated_responses)} responses out of {len(eval_df)} expected.\")\n",
        "  return generated_responses"
      ],
      "metadata": {
        "id": "Rbhx-n81UACo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# le risposte con il modello pre-addestrato\n",
        "print(\"Generating responses with the pre-fine-tuning model...\")\n",
        "generated_responses_pre = generate_responses_pre_finetuning(model, tokenizer, eval_df)"
      ],
      "metadata": {
        "id": "Dv3JAn_JUAEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bert_score import score\n",
        "# Calcola BERTScore per il modello pre-addestrato\n",
        "print(\"Evaluating BERTScore for the pre-fine-tuning model...\")\n",
        "P_pre, R_pre, F1_pre = score(generated_responses_pre, reference_responses, lang=\"it\", verbose=True)\n",
        "print(f\"Pre-fine-tuning BERTScore - Precision: {P_pre.mean().item()}, Recall: {R_pre.mean().item()}, F1: {F1_pre.mean().item()}\")\n"
      ],
      "metadata": {
        "id": "j3-DPJIgUAHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converti i tensori PyTorch in array NumPy, se necessario\n",
        "if isinstance(P_pre, torch.Tensor):\n",
        "    P_pre = P_pre.numpy()\n",
        "if isinstance(R_pre, torch.Tensor):\n",
        "    R_pre = R_pre.numpy()\n",
        "if isinstance(F1_pre, torch.Tensor):\n",
        "    F1_pre = F1_pre.numpy()\n",
        "\n",
        "# Stampa i punteggi medi\n",
        "print(f'Mean Precision: {np.mean(P_pre):.4f}')\n",
        "print(f'Mean Recall: {np.mean(R_pre):.4f}')\n",
        "print(f'Mean F1: {np.mean(F1_pre):.4f}')"
      ],
      "metadata": {
        "id": "9_xWgIhMUAIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.gradient_checkpointing_enable()\n",
        "# Free up memory\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "mLhnDFrnUNwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yaml_config_template = \"\"\"\n",
        "model_type: llm\n",
        "base_model: {model_path}\n",
        "\n",
        "input_features:\n",
        "  - name: DOMANDE\n",
        "    type: text\n",
        "    preprocessing:\n",
        "      max_sequence_length: 300\n",
        "      truncation: true\n",
        "      lowercase: true\n",
        "\n",
        "output_features:\n",
        "  - name: RISPOSTE\n",
        "    type: text\n",
        "    preprocessing:\n",
        "      max_sequence_length: 300\n",
        "      truncation: true\n",
        "    metrics:\n",
        "      - type: bert_score\n",
        "        model: dbmdz/bert-base-italian-uncased\n",
        "      - type: rouge\n",
        "      - type: perplexity\n",
        "\n",
        "prompt:\n",
        "  template: >-\n",
        "    Genera una risposta per la seguente domanda, come se fossi un esperto cardiologo.\n",
        "    Le risposte devono essere empatiche, di senso compiuto e LE FRASI DEVONO TERMINARE CON UN PUNTO e essere lunghe massimo 60 token.\n",
        "    ### Domanda: {DOMANDE}\n",
        "\n",
        "    ### Risposta (Deve terminare con un punto e essere di senso compiuto):\n",
        "\n",
        "generation:\n",
        "  temperature: 0.6\n",
        "  max_new_tokens: 60\n",
        "  top_p: 0.9\n",
        "\n",
        "adapter:\n",
        "  type: lora\n",
        "\n",
        "quantization:\n",
        "  bits: 4\n",
        "\n",
        "preprocessing:\n",
        "  split:\n",
        "    type: fixed\n",
        "\n",
        "trainer:\n",
        "  type: finetune\n",
        "  epochs: 50\n",
        "  batch_size: 16\n",
        "  eval_batch_size: 8\n",
        "  gradient_accumulation_steps: 2\n",
        "  learning_rate: 2.0e-5\n",
        "  enable_gradient_checkpointing: true\n",
        "  weight_decay: 0.01\n",
        "  dropout_rate: 0.1\n",
        "  learning_rate_scheduler:\n",
        "    decay: cosine\n",
        "    warmup_fraction: 0.1\n",
        "    reduce_on_plateau: 0\n",
        "  checkpoint_interval: 500\n",
        "\n",
        "early_stopping:\n",
        "  monitor: val_loss\n",
        "  patience: 5\n",
        "  mode: min\n",
        "\n",
        "logging:\n",
        "  log_to_tensorboard: true\n",
        "\n",
        "callbacks:\n",
        "  - type: progress_bar\n",
        "  - type: model_checkpoint\n",
        "    save_best_only: true\n",
        "    monitor: val_loss\n",
        "    mode: min\n",
        "  - type: early_stopping\n",
        "    monitor: val_loss\n",
        "    patience: 5\n",
        "    mode: min\n",
        "  \"\"\""
      ],
      "metadata": {
        "id": "VmONBubIUNyF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sostituisci solo il segnaposto {model_path}\n",
        "yaml_config = yaml_config_template.replace(\"{model_path}\", model_path)\n",
        "\n",
        "# Carica la configurazione YAML come dizionario\n",
        "qlora_fine_tuning_config = yaml.safe_load(yaml_config)"
      ],
      "metadata": {
        "id": "EsN9bLpyUN03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inizializza il modello Ludwig con la configurazione aggiornata\n",
        "model_ludwig = LudwigModel(config=qlora_fine_tuning_config)"
      ],
      "metadata": {
        "id": "AMShM7MqUN2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Esegui il fine-tuning del modello con Ludwig\n",
        "model_ludwig.train(dataset=df_train)"
      ],
      "metadata": {
        "id": "kvI3kvfGUN5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Lo salvo e lo ricarico per eseguirlo\n",
        "# Salva il modello\n",
        "model_ludwig.save(\"FineTuned_GPT2_Ludwig\")"
      ],
      "metadata": {
        "id": "SlygFIjDUN68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Carica il modello salvato\n",
        "model_loaded = LudwigModel.load(\"FineTuned_GPT2_Ludwig\")"
      ],
      "metadata": {
        "id": "lquYwyPaUAMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bert_score import score\n",
        "\n",
        "# Funzione per generare risposte e calcolare BERTScore\n",
        "def generate_responses_post_finetuning(model, eval_df):\n",
        "    # Ottieni le previsioni dal modello\n",
        "    predictions_df = model.predict(dataset=eval_df)\n",
        "\n",
        "    # Se predictions_df è una tupla, ottieni il DataFrame delle previsioni\n",
        "    if isinstance(predictions_df, tuple):\n",
        "        predictions_df = predictions_df[0]\n",
        "\n",
        "    # Verifica se il DataFrame predictions contiene la colonna 'RISPOSTE_response'\n",
        "    if 'RISPOSTE_response' in predictions_df.columns:\n",
        "        # Estrai le risposte generate\n",
        "        generated_responses = predictions_df['RISPOSTE_response'].apply(lambda x: ''.join(x) if isinstance(x, list) else x).tolist()\n",
        "\n",
        "        for domanda, risposta in zip(eval_df['DOMANDE'], generated_responses):\n",
        "            print(f'\\n[DOMANDA]: {domanda}\\n[GENERATED_TEXT] MODEL_PREDICTION:\\n{risposta}\\n')\n",
        "        return generated_responses\n",
        "    else:\n",
        "        raise ValueError(\"La colonna 'RISPOSTE_response' non è presente nel DataFrame delle previsioni.\")\n"
      ],
      "metadata": {
        "id": "9ESbPDnXUAa6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Genera le risposte con il modello fine-tunato\n",
        "print(\"Generating responses with the post-fine-tuning model...\")\n",
        "generated_responses_post = generate_responses_post_finetuning(model_loaded, eval_df)\n"
      ],
      "metadata": {
        "id": "2dB2ggi1UjMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bert_score import score\n",
        "# Calcola BERTScore per il modello fine-tunato\n",
        "print(\"Evaluating BERTScore for the post-fine-tuning model...\")\n",
        "P_post, R_post, F1_post = score(generated_responses_post, reference_responses, lang=\"it\", verbose=True)\n",
        "print(f\"Post-fine-tuning BERTScore - Precision: {P_post.mean().item()}, Recall: {R_post.mean().item()}, F1: {F1_post.mean().item()}\")"
      ],
      "metadata": {
        "id": "F3xfGBxvUjOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converti i tensori PyTorch in array NumPy, se necessario\n",
        "if isinstance(P_post, torch.Tensor):\n",
        "    P = P_post.numpy()\n",
        "if isinstance(R_post, torch.Tensor):\n",
        "    R = R_post.numpy()\n",
        "if isinstance(F1_post, torch.Tensor):\n",
        "    F1 = F1_post.numpy()\n",
        "\n",
        "# Stampa i punteggi medi\n",
        "print(f'Mean Precision: {np.mean(P):.4f}')\n",
        "print(f'Mean Recall: {np.mean(R):.4f}')\n",
        "print(f'Mean F1: {np.mean(F1):.4f}')"
      ],
      "metadata": {
        "id": "rEdc_jvUUjRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ottieni le previsioni dal modello\n",
        "df_pred = model_loaded.predict(dataset=eval_df)\n",
        "\n",
        "# Se df_pred è una tupla, ottieni il DataFrame delle previsioni\n",
        "if isinstance(df_pred, tuple):\n",
        "   df_pred = df_pred[0]"
      ],
      "metadata": {
        "id": "-M8Il3gDUjam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "# Avvia TensorBoard in background\n",
        "subprocess.Popen(['tensorboard', '--logdir', 'results'])"
      ],
      "metadata": {
        "id": "_d1iPhrXqoBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#FILE JSON:\n",
        "Il file JSON che hai fornito contiene diverse informazioni riguardanti il processo di training del modello. Ecco un'analisi dettagliata dei principali componenti del file:\n",
        "\n",
        "Struttura del File JSON\n",
        "1. evaluation_frequency:\n",
        "  * frequency: Indica la frequenza con cui viene effettuata la valutazione (1 in questo caso, che probabilmente significa ogni epoca).\n",
        "  * period: Indica il periodo della valutazione, in questo caso \"epoch\".\n",
        "\n",
        "2. test: questo campo è vuoto, indicando che non ci sono dati di test disponibili nel file.\n",
        "\n",
        "3. training: contiene informazioni dettagliate sul processo di training per la feature RISPOSTE.\n",
        "\n",
        "  * RISPOSTE:\n",
        "    * loss: Una lista di valori di loss registrati durante il training per ogni epoca.\n",
        "    * next_token_perplexity: Una lista di valori di next_token_perplexity registrati durante il training per ogni epoca.\n",
        "    * perplexity: Una lista di valori di perplexity registrati durante il training per ogni epoca.\n",
        "    * sequence_accuracy: Una lista di valori di sequence_accuracy registrati durante il training per ogni epoca.\n",
        "    * token_accuracy: Una lista di valori di token_accuracy registrati durante il training per ogni epoca.\n",
        "  * combined:\n",
        "    * loss: Una lista di valori di loss combinata registrati durante il training per ogni epoca.\n",
        "4. validation: questo campo è vuoto, indicando che non ci sono dati di validazione disponibili nel file.\n",
        "\n",
        "##Interpretazione delle Metriche\n",
        "* loss: La funzione di perdita (loss) misura quanto bene il modello sta facendo durante il training. Valori più bassi indicano un modello migliore.\n",
        "* next_token_perplexity: La perplexity del prossimo token è una misura della probabilità di una sequenza. Valori più bassi indicano che il modello prevede meglio le sequenze.\n",
        "* perplexity: La perplexity misura quanto bene il modello prevede una sequenza di dati. Valori più bassi indicano prestazioni migliori.\n",
        "* sequence_accuracy: La sequence_accuracy misura la percentuale di sequenze completamente corrette previste dal modello.\n",
        "* token_accuracy: La token_accuracy misura la percentuale di token correttamente previsti dal modello."
      ],
      "metadata": {
        "id": "blVXBvhh6fGe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "# Carica i dati del log\n",
        "log_path = 'results/api_experiment_run/training_statistics.json'\n",
        "\n",
        "with open(log_path, 'r') as f:\n",
        "    log_data = json.load(f)\n",
        "\n",
        "# Verifica la struttura dei dati\n",
        "print(json.dumps(log_data, indent=2))"
      ],
      "metadata": {
        "id": "WZFbiB1DqoC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_loss = log_data['training']['RISPOSTE']['loss']\n",
        "next_token_perplexity = log_data['training']['RISPOSTE']['next_token_perplexity']\n",
        "token_accuracy = log_data['training']['RISPOSTE']['token_accuracy']\n",
        "\n",
        "# Crea un DataFrame con i dati estratti\n",
        "df_metrics = pd.DataFrame({\n",
        "    'epoch': range(1, len(training_loss) + 1),\n",
        "    'train_loss': training_loss,\n",
        "    'next_token_perplexity': next_token_perplexity,\n",
        "    'token_accuracy': token_accuracy\n",
        "})"
      ],
      "metadata": {
        "id": "b_a9L4Qx5PQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Grafico per la loss\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(df_metrics['epoch'], df_metrics['train_loss'], label='Train Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Training Loss')\n",
        "plt.show()\n",
        "\n",
        "# Grafico per next_token_perplexity\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(df_metrics['epoch'], df_metrics['next_token_perplexity'], label='Next Token Perplexity')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Next Token Perplexity')\n",
        "plt.legend()\n",
        "plt.title('Next Token Perplexity')\n",
        "plt.show()\n",
        "\n",
        "# Grafico per token_accuracy\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(df_metrics['epoch'], df_metrics['token_accuracy'], label='Token Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Token Accuracy')\n",
        "plt.legend()\n",
        "plt.title('Token Accuracy')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jR9pxNj_49tO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CONFIGURAZIONE YAML PER VALUTAZIONE GRAFICI:\n",
        "yaml_config_template = \"\"\"\n",
        "\n",
        "model_type: llm\n",
        "\n",
        "base_model: {model_path}\n",
        "\n",
        "\n",
        "input_features:\n",
        "\n",
        "  - name: DOMANDE\n",
        "\n",
        "    type: text\n",
        "\n",
        "    preprocessing:\n",
        "\n",
        "      max_sequence_length: 300\n",
        "\n",
        "      truncation: true     \n",
        "\n",
        "      lowercase: true  \n",
        "\n",
        "output_features:\n",
        "\n",
        "  - name: RISPOSTE\n",
        "\n",
        "    type: text\n",
        "\n",
        "    preprocessing:\n",
        "\n",
        "      max_sequence_length: 300\n",
        "\n",
        "      truncation: true\n",
        "\n",
        "    metrics:\n",
        "\n",
        "      - type: bert_score\n",
        "\n",
        "        model: dbmdz/bert-base-italian-uncased\n",
        "\n",
        "      - type: rouge  # Aggiungi metriche ROUGE per valutare meglio le risposte generate\n",
        "\n",
        "      - type: perplexity  # Aggiungi perplexity come metrica\n",
        "\n",
        "prompt:\n",
        "\n",
        "  template: >-\n",
        "\n",
        "    Genera una risposta per la seguente domanda, come se fossi un esperto cardiologo.\n",
        "    Le risposte devono essere empatiche, di senso compiuto e LE FRASI DEVONO TERMINARE CON UN PUNTO e essere lunghe massimo 60 token.\n",
        "    ### Domanda: {DOMANDE}\n",
        "\n",
        "    ### Risposta (Deve terminare con un punto e essere di senso compiuto):\n",
        "\n",
        "generation:\n",
        "\n",
        "  temperature: 0.6  # Abbassa la temperatura per risposte più coerenti\n",
        "\n",
        "  max_new_tokens: 60\n",
        "\n",
        "  top_p: 0.9  # Aggiungi top_p per migliorare la qualità delle risposte\n",
        "\n",
        "\n",
        "adapter:\n",
        "\n",
        "  type: lora\n",
        "\n",
        "\n",
        "quantization:\n",
        "\n",
        "  bits: 4\n",
        "\n",
        "\n",
        "preprocessing:\n",
        "\n",
        "  split:\n",
        "\n",
        "    type: fixed\n",
        "\n",
        "\n",
        "trainer:\n",
        "\n",
        "  type: finetune\n",
        "\n",
        "  epochs: 100  # Aumenta il numero di epoche per un fine-tuning più approfondito\n",
        "  \n",
        "  batch_size: 8  # Aumenta la dimensione del batch se la memoria lo permette\n",
        "  \n",
        "  eval_batch_size: 4\n",
        "  \n",
        "  gradient_accumulation_steps: 2  # Regola in base alla memoria disponibile\n",
        "  \n",
        "  learning_rate: 3.0e-5  # Prova con un learning rate più basso\n",
        "  \n",
        "  enable_gradient_checkpointing: true\n",
        "  \n",
        "  weight_decay: 0.01  # Aggiungi weight decay per regolarizzare\n",
        "  \n",
        "  dropout_rate: 0.1  # Aggiungi dropout per regolarizzare\n",
        "  \n",
        "  learning_rate_scheduler:\n",
        "  \n",
        "    decay: cosine\n",
        "  \n",
        "    warmup_fraction: 0.1  # Mantieni il warmup fraction\n",
        "  \n",
        "    reduce_on_plateau: 0\n",
        "  \n",
        "  checkpoint_interval: 500  # Salva un checkpoint ogni 500 steps\n",
        "\n",
        "\n",
        "early_stopping:\n",
        "\n",
        "  monitor: val_loss\n",
        "\n",
        "  patience: 3\n",
        "\n",
        "  mode: min\n",
        "\n",
        "\n",
        "logging:\n",
        "\n",
        "  log_to_tensorboard: true\n",
        "\n",
        "\n",
        "callbacks:\n",
        "\n",
        "  - type: progress_bar\n",
        "\n",
        "  - type: model_checkpoint\n",
        "\n",
        "    save_best_only: true\n",
        "\n",
        "    monitor: val_loss\n",
        "\n",
        "    mode: min\n",
        "\n",
        "  - type: early_stopping\n",
        "\n",
        "    monitor: val_loss\n",
        "\n",
        "    patience: 3\n",
        "\n",
        "    mode: min\n",
        "\n",
        "  \"\"\""
      ],
      "metadata": {
        "id": "IyVD_QUn8R58"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#COMMENTO DEI GRAFICI:\n",
        "##TOKEN ACCURACY:\n",
        "Mostra l'andamento dell'accuratezza dei token (Token Accuracy) durante le epoche di addestramento del tuo modello. L'asse X rappresenta il numero di epoche, mentre l'asse Y rappresenta l'accuratezza dei token.\n",
        "* DECLINO PROGRESSIVO:\n",
        "L'accuratezza dei token inizia a circa 0.06 (6%) e diminuisce progressivamente fino a circa 0.02 (2%) verso la fine del training.\n",
        "Questo declino indica che il modello sta diventando meno accurato nel prevedere i token corretti man mano che l'addestramento procede.\n",
        ">>> Possibile OVERFITTING O PROBLEMI DI ADDESTRAMENTO:\n",
        "Un declino continuo e significativo nell'accuratezza dei token può indicare che il modello sta sovraccaricando il rumore presente nel set di dati di training, o potrebbe esserci un problema nella configurazione dell'addestramento.\n",
        "Un'altra possibilità è che il modello stia incontrando difficoltà con il set di dati e non stia apprendendo in modo efficace.\n",
        "\n",
        "**SOLUZIONI:**\n",
        "1. Verifica del Set di Dati:\n",
        "Assicurati che il set di dati di training sia di alta qualità e bilanciato. Dati rumorosi o squilibrati possono influire negativamente sull'addestramento del modello.\n",
        "\n",
        "2. Regolazione dei Parametri di Addestramento:\n",
        "Prova a regolare l'ipotesi di apprendimento (learning rate), la\n",
        "dimensione del batch, e altre iperparametri per migliorare le prestazioni del modello.\n",
        "Potresti voler aggiungere tecniche di regolarizzazione come il dropout per prevenire l'overfitting.\n",
        "\n",
        "3. Early Stopping:\n",
        "Implementa l'early stopping per interrompere l'addestramento se l'accuratezza dei token continua a diminuire, evitando così l'overfitting.\n",
        "\n",
        "5. Analisi dei Modelli di Perdita:\n",
        "Oltre all'accuratezza dei token, osserva anche l'andamento della perdita (loss). Un'analisi combinata delle metriche può fornire ulteriori indicazioni su come migliorare l'addestramento del modello.\n",
        "\n",
        "##NEXT TOKEN PERPLEXITY\n",
        "L'asse X rappresenta il numero di epoche, mentre l'asse Y rappresenta la perplexity del prossimo token. La perplexity misura quanto bene il modello prevede la sequenza di token. Valori più bassi di perplexity indicano una migliore capacità di previsione.\n",
        "\n",
        "* DECLINO PROGRESSIVO: La perplexity inizia intorno a 28500 e diminuisce progressivamente fino a circa 26000 verso la fine del training. Questo declino indica che il modello sta migliorando nella previsione dei token successivi man mano che l'addestramento procede.\n",
        "Il modello sta diventando più preciso nelle sue previsioni.\n",
        "\n",
        "Confrontando questo grafico con quello dell'accuratezza dei token che hai fornito in precedenza, notiamo un contrasto interessante:\n",
        "Accuratezza dei Token: Mostra un declino costante, suggerendo che il modello sta diventando meno preciso nel prevedere i token corretti.\n",
        "Next Token Perplexity: Mostra un declino, suggerendo che il modello sta migliorando nella previsione delle sequenze.\n",
        "\n",
        "##TRAINING LOSS\n",
        "mostra l'andamento della perdita di addestramento (Training Loss) durante le epoche di addestramento del tuo modello. L'asse X rappresenta il numero di epoche, mentre l'asse Y rappresenta il valore della perdita.\n",
        "\n",
        "* DECLINO PROGRESSIVO: La perdita inizia intorno a 7.5 e diminuisce progressivamente fino a circa 4.0 verso la fine del training. Questo declino indica che il modello sta migliorando nell'addestramento, riducendo l'errore nelle sue previsioni.\n",
        "Indica che il modello sta imparando dai dati di training e sta migliorando nel tempo.\n",
        "\n",
        "Considera l'aggiunta di tecniche di regolarizzazione come il dropout o la weight decay per prevenire l'overfitting e migliorare la generalizzazione del modello."
      ],
      "metadata": {
        "id": "Fj8QZeCm79Hj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Comprimi la directory del modello\n",
        "!zip -r FineTuned_GPT_Ludwig.zip FineTuned_GPT2_Ludwig\n",
        "\n",
        "# Scarica il file compresso sul tuo PC\n",
        "files.download('FineTuned_GPT2_Ludwig.zip')"
      ],
      "metadata": {
        "id": "d9DgfhjAUAcr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Salva i risultati in un nuovo file CSV\n",
        "results_dir = 'results'\n",
        "if not os.path.exists(results_dir):\n",
        "    os.makedirs(results_dir)\n",
        "\n",
        "# Percorso del file da salvare\n",
        "file_path = os.path.join(results_dir, 'bertscore_results1.csv')\n",
        "\n",
        "df_pred.to_csv(file_path, index=False)\n",
        "\n",
        "import shutil\n",
        "\n",
        "# Percorso del file zip\n",
        "zip_file = 'results.zip'\n",
        "\n",
        "# Comprime la cartella 'results'\n",
        "shutil.make_archive('results', 'zip', 'results')\n",
        "\n",
        "print(f\"Cartella compressa in: {zip_file}\")\n",
        "\n",
        "# Scarica il file zip\n",
        "files.download(zip_file)"
      ],
      "metadata": {
        "id": "bKUoT8p0UyEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OIkl9saFUyGX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}